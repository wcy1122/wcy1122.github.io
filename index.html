
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Template from Han Guo */

    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 35px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="logo.jpg">
  <title>Chengyao Wang</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="./utils.js"></script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Chengyao Wang (王程钥)</name>
              </p>
              <p>
                I am a PhD student at the Department of Computer Science and Engineering, The Chinese University of Hong Kong (CUHK), advised by <a href="https://jiaya.me/"> Prof. Jiaya Jia </a> and <a href="https://www.cse.cuhk.edu.hk/~byu/"> Prof. Bei Yu </a>. 
                Prior to that, I obtained my B.E. degree in Computer Science from Sun Yat-Sen University (SYSU).
                <br>
                <br>
                I am particular interested in building <span style="color: #ff0000;"><b>Multimodal Intelligence</b></span> that can actively interact with the physical world and continuously learning from interaction.
                My recent research mainly focus on the the former part, building Multi-modal Large Language Models (MLLMs) to make Intelligence capable of interacting with the physical world.
                Representative works includes <a href="https://arxiv.org/abs/2311.17043"> LLaMA-VID </a>, <a href="https://arxiv.org/abs/2403.18814"> Mini-Gemini </a> and <a href="https://arxiv.org/abs/2509.25131"> MGM-Omni </a>.
                Prior to that, I also had some experience on visual perception and representation learning.
                <br>
                <br>
                I am seeking <span style="color: #ff0000;"><b>Research Scientist / Member of Technical Staff</b></span> opportunities in industry for <span style="color: #ff0000;"><b>2026 Fall</b></span> on <span style="color: #ff0000;"><b>Multimodal Foundation Agents</b></span>, including Foundation MLLMs, Computer Use Agents and Embodied AI, open to any location. Feel free to contact if there is a good match.
                Research discussion and collaboration are always welcome, feel free to set up a coffee chat.
              </p>
              <p align=center>
                <a href="https://scholar.google.com/citations?user=1pZcoqgAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/wcy1122"> GitHub </a> &nbsp/&nbsp
                <a href="https://x.com/wcy1122"> X </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chengyao-wang-7487211a1/"> Linkdin </a> &nbsp/&nbsp
                <a href="mailto: wcy1122@link.cuhk.edu.hk"> Email </a>
              </p>
            </td>
            <td width="30%">
              <img src="selfie.png" style="width: 80%;">
            </td>
          </tr>
        </table>

        <!-- News -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="60%" valign="middle">
              <heading>News</heading>
              <ul>
                <li> <b>[2025-11]</b> Finally, <a href="https://arxiv.org/abs/2403.18814"> Mini-Gemini </a> is accepted by TPAMI, cheers! </li>
                <li> <b>[2025-08]</b> We release <a href="https://arxiv.org/abs/2509.25131"> MGM-Omni </a>, an open source omni moded support long speech understanding, generation and zero-shot voice clone. </li>
                <li> <b>[2025-08]</b> <a href="https://arxiv.org/abs/2510.23607"> Concerto </a> is accepted in NeurIPS 2025, San Diego. </li>
                <li> <b>[2025-06]</b> <a href="https://arxiv.org/abs/2412.09501"> Lyra </a> is accepted in ICCV 2025, Hawaii. </li>
                <li> <b>[2025-03]</b> <a href="https://arxiv.org/abs/2412.04467"> VisionZip </a> and <a href="https://arxiv.org/abs/2412.17098"> DreamOmni </a> are accepted in CVPR 2025, Nashville. </li>
                <li> <b>[2024-12]</b> We release <a href="https://arxiv.org/abs/2412.09501"> Lyra </a>, an open source multi-modal large language models that support long speech comprehension, omni understanding and cross-modality efficiency. </li>
                <li> <b>[2024-07]</b> <a href="https://arxiv.org/abs/2311.17043"> LLaMA-VID </a> is accepted in ECCV 2024, Milano. </li>
                <li> <b>[2024-03]</b> We release <a href="https://arxiv.org/abs/2403.18814"> Mini-Gemini </a>, an open source vision-language models that support high-resolution image understanding and reasoning image generation. </li>
                <li> <b>[2024-02]</b> <a href="https://arxiv.org/abs/2403.09639"> GroupContrast </a> is accepted in CVPR 2024, Seattle. </li>
                <li> <b>[2023-11]</b> We release <a href="https://arxiv.org/abs/2311.17043"> LLaMA-VID </a>, an open source vision-language models that support hour-long video understanding and reasoning. </li>
              </ul>
            </td>
          </tr>
        </table>

        <!-- Researches -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research </heading>
              <p>
              * indicates equal contribution
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Omni-modal Large Language Models (Omni MLLMs) </heading>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <!-- Projects -->
          <tr>
            <td width="25%" id="mgm-omni-img"></td>
            <td valign="top" width="75%" id="mgm-omni-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "mgm-omni", {
              title: "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
              paper_url: "https://arxiv.org/abs/2509.25131",
              authors: "<b> Chengyao Wang* </b>, Zhisheng Zhong*, Bohao Peng*, Senqiao Yang, Yuqi Liu, Haokun Gui, Bin Xia, Jingyao Li, Bei Yu, Jiaya Jia",
              conference: "Arxiv preprint",
              image: "figures/MGM-Omni.jpg",
              others: "[<a href=\"https://arxiv.org/pdf/2509.25131.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/MGM-Omni\"><b>code</b></a>] [<a href=\"https://huggingface.co/spaces/wcy1122/MGM-Omni\"><b>demo</b></a>]"
            });
          </script>

          <tr>
            <td width="25%" id="lyra-img"></td>
            <td valign="top" width="75%" id="lyra-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "lyra", {
              title: "Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition",
              paper_url: "https://arxiv.org/abs/2412.09501",
              authors: "Zhisheng Zhong*, <b> Chengyao Wang* </b>, Yuqi Liu*, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia",
              conference: "ICCV 2025",
              image: "figures/Lyra.png",
              others: "[<a href=\"https://arxiv.org/pdf/2412.09501.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/Lyra\"><b>code</b></a>] [<a href=\"https://lyra-omni.github.io/\"><b>project</b></a>] [<a href=\"https://103.170.5.190:17860/\"><b>demo</b></a>]"
            });
          </script>
        
        </tabel>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Vision Language Models (VLMs) </heading>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%" id="visionzip-img"></td>
            <td valign="top" width="75%" id="visionzip-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "visionzip", {
              title: "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
              paper_url: "https://arxiv.org/abs/2412.04467",
              authors: "Senqiao Yang, Yukang Chen, Zhuotao Tian, <b> Chengyao Wang </b>, Jingyao Li, Bei Yu, Jiaya Jia",
              conference: "CVPR 2025",
              image: "figures/VisionZip.png",
              others: "[<a href=\"https://arxiv.org/pdf/2412.04467.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/VisionZip\"><b>code</b></a>] [<a href=\"http://202.104.135.156:7860/\"><b>demo</b></a>]"
            });
          </script>
          
          <tr>
            <td width="25%" id="minigemini-img"></td>
            <td valign="top" width="75%" id="minigemini-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "minigemini", {
              title: "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models",
              paper_url: "https://arxiv.org/abs/2403.18814",
              authors: "Yanwei Li*, Yuechen Zhang*, <b> Chengyao Wang* </b>, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia,",
              conference: "TPAMI 2025",
              image: "figures/MiniGemini.png",
              others: "[<a href=\"https://arxiv.org/pdf/2403.18814.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/MiniGemini\"><b>code</b></a>] [<a href=\"https://mini-gemini.github.io/\"><b>project</b></a>] [<a href=\"http://103.170.5.190:7860/\"><b>demo</b></a>]"
            });
          </script>

          <tr>
            <td width="25%" id="llamavid-img"></td>
            <td valign="top" width="75%" id="llamavid-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "llamavid", {
              title: "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models",
              paper_url: "https://arxiv.org/abs/2311.17043",
              authors: "Yanwei Li*, <b> Chengyao Wang* </b>, Jiaya Jia",
              conference: "ECCV 2024",
              image: "figures/LLaMA-VID.png",
              others: "[<a href=\"https://arxiv.org/pdf/2311.17043.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/LLaMA-VID\"><b>code</b></a>] [<a href=\"https://llama-vid.github.io/\"><b>project</b></a>] [<a href=\"http://103.170.5.190:7864/\"><b>demo</b></a>]"
            });
          </script>
        
        </tabel>  
        

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Visual Generation (AIGC) </heading>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%" id="dreamomni2-img"></td>
            <td valign="top" width="75%" id="dreamomni2-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "dreamomni2", {
              title: "DreamOmni2: Multimodal Instruction-based Editing and Generation",
              paper_url: "https://arxiv.org/abs/2510.06679",
              authors: "Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, <b> Chengyao Wang </b>, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia",
              conference: "Arxiv Preprint",
              image: "figures/DreamOmni2.png",
              others: "[<a href=\"https://arxiv.org/pdf/2510.06679.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/DreamOmni2\"><b>code</b></a>] [<a href=\"https://pbihao.github.io/projects/DreamOmni2/index.html\"><b>project</b></a>] [<a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit\"><b>demo1</b></a>]  [<a href=\"https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen\"><b>demo2</b></a>]"
            });
          </script>
          
          <tr>
            <td width="25%" id="dreamomni-img"></td>
            <td valign="top" width="75%" id="dreamomni-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "dreamomni", {
              title: "DreamOmni: Unified Image Generation and Editing",
              paper_url: "https://arxiv.org/abs/2412.17098",
              authors: "Bin Xia, Yuechen Zhang, Jingyao Li, <b> Chengyao Wang </b>, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia",
              conference: "CVPR 2025",
              image: "figures/DreamOmni.png",
              others: "[<a href=\"https://arxiv.org/pdf/2412.17098.pdf\"><b>paper</b></a>] [<a href=\"https://zj-binxia.github.io/DreamOmni-ProjectPage\"><b>project</b></a>]"
            });
          </script>

        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Visual Perception & Representation </heading>
          </tr>
        </table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="25%" id="concerto-img"></td>
            <td valign="top" width="75%" id="concerto-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "concerto", {
              title: "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
              paper_url: "https://arxiv.org/abs/2510.23607",
              authors: "Yujia Zhang, Xiaoyang Wu, Yixing Lao, <b> Chengyao Wang </b>, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao",
              conference: "NeurIPS 2025",
              image: "figures/Concerto.png",
              others: "[<a href=\"https://arxiv.org/pdf/2510.23607.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/Pointcept/Concerto\"><b>code</b></a>] [<a href=\"https://pointcept.github.io/Concerto/\"><b>project</b></a>] [<a href=\"https://huggingface.co/spaces/Pointcept/Concerto\"><b>demo</b></a>]"
            });
          </script>

          <tr>
            <td width="25%" id="groupcontrast-img"></td>
            <td valign="top" width="75%" id="groupcontrast-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "groupcontrast", {
              title: "GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding",
              paper_url: "https://arxiv.org/abs/2403.09639",
              authors: "<b> Chengyao Wang </b>, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, Jiaya Jia",
              conference: "CVPR 2024",
              image: "figures/GroupContrast.png",
              others: "[<a href=\"https://arxiv.org/pdf/2403.09639.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/dvlab-research/GroupContrast\"><b>code</b></a>]"
            });
          </script>

          <tr>
            <td width="25%" id="hdmnet-img"></td>
            <td valign="top" width="75%" id="hdmnet-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "hdmnet", {
              title: "Hierarchical Dense Correlation Distillation for Few-Shot Segmentation",
              paper_url: "https://arxiv.org/abs/2303.14652",
              authors: "Bohao Peng, Zhuotao Tian, Xiaoyang Wu, <b> Chengyao Wang </b>, Shu Liu, Jingyong Su, Jiaya Jia",
              conference: "CVPR 2023 (Highlight)",
              image: "figures/HDMNet.png",
              others: "[<a href=\"https://arxiv.org/pdf/2303.14652.pdf\"><b>paper</b></a>] [<a href=\"https://github.com/Pbihao/HDMNet\"><b>code</b></a>]"
            });
          </script>
          
          <tr>
            <td width="25%" id="msil-img"></td>
            <td valign="top" width="75%" id="msil-txt"></td>
          </tr>
          <script type="text/javascript">
            createProjectElement(
              id = "msil", {
              title: "Mixed supervision for instance learning in object detection with few-shot annotation",
              paper_url: "https://dl.acm.org/doi/abs/10.1145/3503161.3548242",
              authors: "Yi Zhong*, <b> Chengyao Wang* </b>, Shiyong Li, Zhu Zhou, Yaowei Wang, Wei-Shi Zheng",
              conference: "ACM MM 2022",
              image: "figures/MSIL.png",
              others: "[<a href=\"https://drive.google.com/file/d/1AGChiG00dIMrvqPhtChj26uE_Etz3Vxj/view\"><b>paper</b></a>]"
            });
          </script>

        </tabel>

        <!-- Awards -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="60%" valign="middle">
              <heading>Selected Awards</heading>
              <ul>
                <li> Gold Medal x 3, <b>International Collegiate Programming Contest (ICPC), Regional</b> </li>
                <li> Gold Medal, <b>Chinese Collegiate Programming Contest (CCPC)</b> </li>
              </ul>
            </td>
          </tr>
        </table>

        <!-- Academic Service -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="60%" valign="middle">
              <heading>Academic Service</heading>
              <p>
                <b>Reviewer / Program Committee Member</b>
              </p>
              <ul>
                <li> IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </li>
                <li> IEEE International Conference on Computer Vision (ICCV) </li>
                <li> Conference on Neural Information Processing Systems (NeurIPS) </li>
                <li> International Conference on Learning Representations (ICLR) </li>
                <li> Association for the Advancement of Artificial Intelligence (AAAI) </li>
                <li> IEEE Winter Conference on Applications of Computer Vision (WACV) </li>
              </ul>
            </td>
          </tr>
        </table>

        <table
          style="width:50%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:30%;vertical-align:middle">
                <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=HwqhcZKNc3n2Az2F-fAc4sG-jBpdLe8XOiMForJ2a9M'></script>
              </td>
            </tr>
          </tbody>
        </table>

      </td>
    </tr>
  </tabel>
</body>

</html>
